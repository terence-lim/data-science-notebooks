{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a3a24b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# LLM Fine-tuning\n",
    "\n",
    "*To improve is to change; to be perfect is to change often* - Winston Churchill\n",
    "\n",
    "Large language models (LLMs) have demonstrated remarkable general capabilities, but tailoring them to specific tasks or domains may require fine-tuning -- adjusting model weights by further training on task-specific data. We examine the fine-tuning of Meta’s **Llama-3.1** model using tools from the Hugging Face ecosystem, applying efficient techniques such as quantization and low-rank adaptation (LoRA) to an industry text classification task using firm-level 10-K filings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b7ef95",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# By: Terence Lim, 2020-2025 (terence-lim.github.io)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import textwrap\n",
    "import warnings\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from finds.database import SQL, RedisDB\n",
    "from finds.unstructured import Edgar\n",
    "from finds.structured import BusDay, CRSP, PSTAT\n",
    "from finds.readers import Sectoring\n",
    "from finds.utils import Store\n",
    "from secret import paths, CRSP_DATE, credentials\n",
    "logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e8dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_EPOCHS = 2   # 0 # 1\n",
    "RESUME_FROM_CHECKPOINT = False   # False # True\n",
    "MAX_SEQ_LENGTH = 1024  #512 #2048\n",
    "LOGGING_STEPS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa18b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(permnos)=3474\n"
     ]
    }
   ],
   "source": [
    "VERBOSE = 0\n",
    "sql = SQL(**credentials['sql'], verbose=VERBOSE)\n",
    "bd = BusDay(sql)\n",
    "rdb = RedisDB(**credentials['redis'])\n",
    "crsp = CRSP(sql, bd, rdb, verbose=VERBOSE)\n",
    "pstat = PSTAT(sql, bd, verbose=VERBOSE)\n",
    "ed = Edgar(paths['10X'], zipped=True, verbose=0)\n",
    "store = Store('assets', ext='pkl')\n",
    "permnos = list(store.load('nouns').keys()) \n",
    "print(f\"{len(permnos)=}\")   # comparable sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2b4341",
   "metadata": {},
   "source": [
    "\n",
    "## Meta Llama-3.1 model\n",
    "\n",
    "Meta’s **Llama 3.1** is an open-source large language model released in July 2024 under the Llama 3.1 Community License, permitting broad use, including commercial applications. Key highlights include:\n",
    "- Model variants:\n",
    "  - 8B: 8 billion parameters.\n",
    "  - 70B: 70 billion parameters.\n",
    "  - 405B: 405 billion parameters.\n",
    "- Context length of up to 128,000 tokens.\n",
    "- Pre-trained on over 15 trillion tokens sourced from publicly available datasets.\n",
    "- Fine-tuned using supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF).\n",
    "- Multilingual support, including English, French, German, Hindi, Italian, Portuguese, Spanish, and Thai.\n",
    "\n",
    "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2886a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'meta-llama/Llama-3.1-8B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c01333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3080 Laptop GPU. Max memory = 15.739 GB.\n"
     ]
    }
   ],
   "source": [
    "# Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "max_memory = round(gpu_stats.total_memory / (1024**3), 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "\n",
    "def cuda_memory(title, trainer_stats=None):\n",
    "    \"\"\"Show final memory and optional trainer stats\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        total_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "        reserved_memory = torch.cuda.memory_reserved(device)\n",
    "        allocated_memory = torch.cuda.memory_allocated(device)\n",
    "        free_memory = total_memory - reserved_memory\n",
    "        print(f'------ {title.upper()} ------')\n",
    "        if trainer_stats:\n",
    "            print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "        print(f\"Total memory: {total_memory / (1024**3):.2f} GB\")\n",
    "        print(f\"Reserved memory: {reserved_memory / (1024**3):.2f} GB\")\n",
    "        print(f\"Allocated memory: {allocated_memory / (1024**3):.2f} GB\")\n",
    "        print(f\"Free memory: {free_memory / (1024**3):.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11576671",
   "metadata": {},
   "source": [
    "\n",
    "## Supervised fine-tuning (SFT)\n",
    "\n",
    "Supervised Fine-Tuning is the process of enhancing a pre-trained language model by fine-tuning it on labeled input–output pairs using standard supervised learning. Common use cases include:\n",
    "- Instruction tuning: The model learns to follow new instructions\n",
    "- Chatbot fine-tuning (e.g., with help-desk data)\n",
    "- Domain adaptation (e.g., legal, medical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3891b694",
   "metadata": {},
   "source": [
    "### Huggingface framework\n",
    "\n",
    "Several ecosystems support fine-tuning and training of LLMs. The Hugging Face Ecosystem includes:\n",
    "- `transformers`: Model architectures and training components.\n",
    "- Transformers Reinforcement Learning (`trl`): Training large language models (LLMs) with reinforcement learning techniques, especially for alignment tasks like RLHF (Reinforcement Learning with Human Feedback) and DPO (Direct Preference Optimization).\n",
    "- `bitsandbytes`: Enables efficient low-bit model quantization, allowing large language models to run on limited GPU memory without much loss in performance.\n",
    "- Parameter-Efficient Fine-Tuning (`peft`): Tools to fine-tune large language models by training only a small number of additional parameters.\n",
    "- Accelerate: Distributed training optimization.\n",
    "- `datasets`: For loading, processing, and managing datasets\n",
    "\n",
    "It provides access to 100k+ pre-trained transformer models, and tools for efficient-tuning of these models using low memory and quantized weights.\n",
    "\n",
    "If you encounter a gated model repository on Hugging Face, it means the model requires manual access approval from the authors before you can use or download it. You should log in to your huggingface.ro account, go to the Model Page, and click on the \"Request Access\" button -- approval may take up to a few days. When authorized, make sure you have set your Hugging Face token in your environment (e.g. `huggingface-cli login`), see https://huggingface.co/settings/tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e0d3594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locations to save fine-tuned model weights\n",
    "output_dir = str(Path(paths['scratch'], \"fine-tuned-model\"))   # training checkpoints\n",
    "model_dir = str(Path(paths['scratch'], \"Llama-3.1-8B-Instruct-FF-Sector\"))  # final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eacaf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "args = SFTConfig(\n",
    "    output_dir=output_dir,                    # directory to save and repository id\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS, ####1  # number of training epochs\n",
    "    per_device_train_batch_size=2,    ####1   # batch size per device during training\n",
    "    gradient_accumulation_steps=4,  ####8     # before performing a backward/update pass\n",
    "    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_strategy=\"steps\",                 # or \"steps\" or \"no\" or \"epoch\"\n",
    "    logging_steps=LOGGING_STEPS, #### 1,                         \n",
    "    learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n",
    "    report_to=\"tensorboard\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,  #512,  ### should be 1024? or MAX_CHARS // 4\n",
    "    packing=False,\n",
    "    dataset_kwargs={\n",
    "    \"add_special_tokens\": False,\n",
    "    \"append_concat_token\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c911cb",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "The `AutoTokenizer` in Hugging Face is a smart utility that automatically loads the correct tokenizer for a given pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c54e861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and set the pad token id. \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde437b6",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "\n",
    "Quantization converts high-precision data to lower-precision data, for instance, by representing model weights and activation values as 4-bit or 8-bit integers instead of 32-bit floating point numbers. The `bitsandbytes` library for efficient low-bit model quantization is integrated with Hugging Face and works seamlessly with parameter-efficient fine-tuning like QLora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Llama-3.1-8b-instruct model in 4-bit quantization to save GPU memory\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e1a29",
   "metadata": {},
   "source": [
    "### AutoModel\n",
    "\n",
    "The `AutoModel` class in Hugging Face is a convenient interface that automatically loads the correct model architecture based on the model name or path. Its variants automatically load the correct model head (e.g., classification layer, decoder head) based on your specific task, e.g.\n",
    "\n",
    "| Class | Task | Output |\n",
    "|-------|------|--------|\n",
    "| `AutoModel` | Base model (no head) | Hidden states |\n",
    "| `AutoModelForSequenceClassification` | Text classification (e.g. sentiment) | Class logits |\n",
    "| `AutoModelForTokenClassification` | Token labeling (e.g. NER, POS) | Token-level logits |\n",
    "| `AutoModelForQuestionAnswering` | Extractive QA | Start/end logits for answer spans |\n",
    "| `AutoModelForCausalLM` | Text generation (GPT-style) | Next-token logits |\n",
    "| `AutoModelForMaskedLM` | Mask filling (BERT-style) | Predictions for masked tokens |\n",
    "| `AutoModelForSeq2SeqLM` | Translation, summarization (T5, BART) | Generated sequences |\n",
    "| `AutoModelForMultipleChoice` | Multiple-choice QA (e.g. SWAG) | Choice logits |\n",
    "| `AutoModelForVision2Seq` | Image captioning | Generated text |\n",
    "| `AutoModelForImageClassification` | Vision tasks | Class logits |\n",
    "| `AutoModelForSpeechSeq2Seq` | Speech translation | Generated text from audio |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4419f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38bf42a",
   "metadata": {},
   "source": [
    "### Parameter-efficient fine-tuning\n",
    "\n",
    "**Parameter-Efficient Fine-Tuning (PEFT)** is both a technique and a Hugging Face library for adapting large language models (LLMs) to new tasks by training only a small subset of parameters. Instead of updating the entire model, the base (pretrained) model is kept frozen, and lightweight, trainable components called **adapters** are added. These adapters typically involve only a few million parameters, making fine-tuning faster and more memory-efficient.\n",
    "\n",
    "- **Low-rank factorization**: This is a compression technique which decomposes a large matrix of weights into a smaller, lower-rank matrix, resulting in a more compact approximation that requires fewer parameters and computations.\n",
    "\n",
    "- **LoRA**: A small number of trainable low-rank matrices are added to the model's attention layers. The original weights are frozen and just these adapters are fine-tuned.\n",
    "\n",
    "- **QLora**: Combines LoRA with Quantization: The base model is converted to 4-bit precision, reducing memory usage dramatically without losing much performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a48db629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q_proj', 'down_proj', 'v_proj', 'gate_proj', 'o_proj', 'up_proj', 'k_proj']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the linear module names from the model using the bits and bytes library. \n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "modules = find_all_linear_names(model)\n",
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58b4a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA for the target modules, task type, and other training arguments \n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4f6ef",
   "metadata": {},
   "source": [
    "## Industry text classification\n",
    "\n",
    "We fine-tune the model for classifying firms into ten Fama-French sector categories based on their business descriptions in 10-K filings. The text data for each U.S.-domiciled common stock is drawn from the most recent year's Business Description section of their 10-K filings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b26a084",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Load 10-K business description text for industry classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46c239c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beg=20240102, end=20241231\n",
      "class_labels=array(['Durbl', 'Enrgy', 'HiTec', 'Hlth', 'Manuf', 'NoDur', 'Other',\n",
      "       'Shops', 'Telcm', 'Utils'], dtype=object)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve universe of stocks\n",
    "beg, end = bd.begyr(CRSP_DATE), bd.endyr(CRSP_DATE)\n",
    "print(f\"{beg=}, {end=}\")\n",
    "univ = crsp.get_universe(bd.endyr(CRSP_DATE, -1))\n",
    "\n",
    "# lookup company names\n",
    "comnam = crsp.build_lookup(source='permno', target='comnam', fillna=\"\")\n",
    "univ['comnam'] = comnam(univ.index)\n",
    "\n",
    "# lookup company names\n",
    "comnam = crsp.build_lookup(source='permno', target='comnam', fillna=\"\")\n",
    "univ['comnam'] = comnam(univ.index)\n",
    "\n",
    "# lookup ticker symbols\n",
    "ticker = crsp.build_lookup(source='permno', target='ticker', fillna=\"\")\n",
    "univ['ticker'] = ticker(univ.index)\n",
    "\n",
    "# lookup sic codes from Compustat, and map to FF 10-sector code\n",
    "sic = pstat.build_lookup(source='lpermno', target='sic', fillna=0)\n",
    "industry = Series(sic[univ.index], index=univ.index)\n",
    "industry = industry.where(industry > 0, univ['siccd'])\n",
    "sectors = Sectoring(sql, scheme='codes10', fillna='')   # supplement from crosswalk\n",
    "univ['sector'] = sectors[industry]\n",
    "\n",
    "# retrieve latest year's bus10K's\n",
    "item, form = 'bus10K', '10-K'\n",
    "rows = DataFrame(ed.open(form=form, item=item))\n",
    "rows = rows[rows['date'].between(beg, end)]\\\n",
    "    .drop_duplicates(subset=['permno'], keep='last')\\\n",
    "    .set_index('permno')\\\n",
    "    .reindex(permnos)\n",
    "\n",
    "# split documents into train/test sets\n",
    "labels = univ.loc[permnos, 'sector']\n",
    "class_labels = np.unique(labels)\n",
    "print(f\"{class_labels=}\")\n",
    "\n",
    "train_index, test_index = train_test_split(permnos,\n",
    "                                           stratify=labels,\n",
    "                                           random_state=42,\n",
    "                                           test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a493aa1",
   "metadata": {},
   "source": [
    "### HuggingFace `dataset` module\n",
    "\n",
    "The training data are converted to LLM instruction statements, and implemented as a HuggingFace Dataset class. This class can be conveniently created from many different sources, including data files of various formats or from a generator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "123c77db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLM instruction statement\n",
    "MAX_CHARS = MAX_SEQ_LENGTH * 2\n",
    "class_text = \"'\" + \"' or '\".join(class_labels) + \"'\"\n",
    "def generate_prompt(permno, test=False):\n",
    "    text = ed[rows.loc[permno, 'pathname']].replace('\\n','')[:MAX_CHARS]\n",
    "    return f\"\"\"\n",
    "Classify the text into one of these {len(class_labels)} classification labels:\n",
    "{class_text} \n",
    "and return the answer as the label.\n",
    "text: {text}\n",
    "label: {'' if test else univ.loc[permno, 'sector']}\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff2911fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ BEFORE DATASET ------\n",
      "Total memory: 15.74 GB\n",
      "Reserved memory: 6.83 GB\n",
      "Allocated memory: 5.63 GB\n",
      "Free memory: 8.91 GB\n"
     ]
    }
   ],
   "source": [
    "cuda_memory('before dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a5061da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the text into one of these 10 classification labels: 'Durbl'\n",
      "or 'Enrgy' or 'HiTec' or 'Hlth' or 'Manuf' or 'NoDur' or 'Other' or\n",
      "'Shops' or 'Telcm' or 'Utils'  and return the answer as the label.\n",
      "text: ITEM 1. BUSINESS  OVERVIEW  B. RILEY FINANCIAL, INC. (NASDAQ:\n",
      "RILY) (THE COMPANY IS A DIVERSIFIED FINANCIAL SERVICES PLATFORM THAT\n",
      "DELIVERS TAILORED SOLUTIONS TO MEET THE STRATEGIC, OPERATIONAL, AND\n",
      "CAPITAL NEEDS OF ITS CLIENTS AND PARTNERS. WE OPERATE THROUGH SEVERAL\n",
      "CONSOLIDATED SUBSIDIARIES (COLLECTIVELY, B. RILEY THAT PROVIDE\n",
      "INVESTMENT BANKING, BROKERAGE, WEALTH MANAGEMENT, ASSET MANAGEMENT,\n",
      "DIRECT LENDING, BUSINESS ADVISORY, VALUATION, AND ASSET DISPOSITION\n",
      "SERVICES TO A BROAD CLIENT BASE SPANNING PUBLIC AND PRIVATE COMPANIES,\n",
      "FINANCIAL SPONSORS, INVESTORS, FINANCIAL INSTITUTIONS, LEGAL AND\n",
      "PROFESSIONAL SERVICES FIRMS, AND INDIVIDUALS.   THE COMPANY\n",
      "OPPORTUNISTICALLY INVESTS IN AND ACQUIRES COMPANIES OR ASSETS WITH\n",
      "ATTRACTIVE RISK-ADJUSTED RETURN PROFILES TO BENEFIT OUR SHAREHOLDERS.\n",
      "WE OWN AND OPERATE SEVERAL UNCORRELATED CONSUMER BUSINESSES AND INVEST\n",
      "IN BRANDS ON A PRINCIPAL BASIS. OUR APPROACH IS FOCUSED ON HIGH\n",
      "QUALITY COMPANIES AND ASSETS IN INDUSTRIES IN WHICH WE HAVE EXTENSIVE\n",
      "KNOWLEDGE AND CAN BENEFIT FROM OUR EXPERIENCE TO MAKE OPERATIONAL\n",
      "IMPROVEMENTS AND MAXIMIZE FREE CASH FLOW. OUR PRINCIPAL INVESTMENTS\n",
      "OFTEN LEVERAGE THE FINANCIAL, RESTRUCTURING, AND OPERATIONAL EXPERTISE\n",
      "OF OUR PROFESSIONALS WHO WORK COLLABORATIVELY ACROSS DISCIPLINES.   WE\n",
      "REFER TO B. RILEY AS A PLATFORM BECAUSE OF THE UNIQUE COMPOSITION OF\n",
      "OUR BUSINESS. OUR PLATFORM HAS GROWN CONSIDERABLY AND BECOME MORE\n",
      "DIVERSIFIED OVER THE PAST SEVERAL YEARS. WE HAVE INCREASED OUR MARKET\n",
      "SHARE AND EXPANDED THE DEPTH AND BREADTH OF OUR BUSINESSES BOTH\n",
      "ORGANICALLY AND THROUGH OPPORTUNISTIC ACQUISITIONS. OUR INCREASINGLY\n",
      "DIVERSIFIED PLATFORM ENABLES US TO INVEST OPPORTUNISTICALLY AND TO\n",
      "DELIVER STRONG LONG-TERM INVESTMENT PERFORMANCE THROUGHOUT A RANGE OF\n",
      "ECONOMIC CYCLES.   OUR PLATFORM IS COMPRISED OF MORE THAN 2,700\n",
      "AFFILIATED PROFESSIONALS, INCLUDING EMPLOYEES AND INDEPENDENT\n",
      "CONTRACTORS. WE ARE HEADQUARTERED IN LOS ANGELES, CALIFORNIA AND\n",
      "MAINTAIN OFFICES THROUGHOUT THE U.S., INCLUDING IN NEW YORK, CHICAGO,\n",
      "METRO DISTRICT OF COLUMBIA, AT label: Other\n"
     ]
    }
   ],
   "source": [
    "X_train = DataFrame(columns=['text'], index=train_index,\n",
    "                    data=[generate_prompt(permno, test=False) for permno in train_index])\n",
    "X_test = DataFrame(columns=['text'], index=test_index,\n",
    "                   data=[generate_prompt(permno, test=True) for permno in test_index])\n",
    "y_test = [univ.loc[permno, 'sector'] for permno in test_index]\n",
    "\n",
    "train_data = Dataset.from_pandas(X_train[[\"text\"]])\n",
    "test_data = Dataset.from_pandas(X_test[[\"text\"]])\n",
    "print(textwrap.fill(train_data['text'][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a28675e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820 MAX_SEQ_LENGTH=1024\n"
     ]
    }
   ],
   "source": [
    "# verify max_seq_length sufficient\n",
    "curr_max = 0\n",
    "for row, data in enumerate(train_data):\n",
    "    tokenized = tokenizer.tokenize(data['text'])\n",
    "    curr_max = max(curr_max, len(tokenized))\n",
    "#    print(f\"{row=}, {len(tokenized)=}\")\n",
    "assert curr_max < args.max_seq_length\n",
    "print(curr_max, f\"{MAX_SEQ_LENGTH=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "874c5a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ AFTER DATASET ------\n",
      "Total memory: 15.74 GB\n",
      "Reserved memory: 6.83 GB\n",
      "Allocated memory: 5.63 GB\n",
      "Free memory: 8.91 GB\n"
     ]
    }
   ],
   "source": [
    "cuda_memory('after dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90442405",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Hugging Face's `pipeline` function enables one-line use for easy inference, by simply specifying the model, tokenizer, generation parameters (e.g. sampling methdology, maximum new tokens), and task, e.g.:\n",
    "- \"text-classification\": Sentiment analysis, topic labeling\n",
    "- \"token-classification\": Named Entity Recognition (NER), POS tagging\n",
    "- \"question-answering\": Extractive QA from context\n",
    "- \"text-generation\": Generate text (GPT-style)\n",
    "- \"summarization\": Generate summaries from long text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fc9715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the text generation pipeline to predict labels from the “text” \n",
    "def generate(prompt, model=model, tokenizer=tokenizer, verbose=False):\n",
    "    \"\"\"Generate a response\"\"\"\n",
    "    pipe = pipeline(task=\"text-generation\", \n",
    "                    model=model, \n",
    "                    tokenizer=tokenizer,\n",
    "                    do_sample=False,\n",
    "                    top_p=None,\n",
    "                    top_k=None,\n",
    "                    return_full_text=False,\n",
    "                    max_new_tokens=4,   # 2\n",
    "                    temperature=None)    # 0.1        \n",
    "    result = pipe(prompt)\n",
    "    answer = result[0]['generated_text'].split(\"label:\")[-1].strip()\n",
    "    if verbose:\n",
    "        print(f\"{len(prompt)=}, {result=}, {answer=}\")\n",
    "    return answer\n",
    "\n",
    "def predict(test, model, tokenizer, verbose=False):\n",
    "    \"\"\"Predict test set\"\"\"\n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(test))):\n",
    "        prompt = test.iloc[i][\"text\"]\n",
    "        answer = generate(prompt, model, tokenizer, verbose=verbose)\n",
    "        # Determine the predicted category\n",
    "        for category in class_labels:\n",
    "            if category.lower() in answer.lower():\n",
    "                y_pred.append(category)\n",
    "                break\n",
    "        else:\n",
    "            y_pred.append(\"none\")\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a8fdeb",
   "metadata": {},
   "source": [
    "Create function that will use the predicted labels and true labels to compute the overall accuracy, classification report, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54c1f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    mapping = {label: idx for idx, label in enumerate(class_labels)}\n",
    "\n",
    "    def map_func(x):\n",
    "        return mapping.get(x, -1)  # Map to -1 if not found, should not occur with correct data\n",
    "    \n",
    "    y_true_mapped = np.vectorize(map_func)(y_true)\n",
    "    y_pred_mapped = np.vectorize(map_func)(y_pred)\n",
    "    labels = list(mapping.values())\n",
    "    target_names = list(mapping.keys())\n",
    "    if -1 in y_pred_mapped:\n",
    "        labels += [-1]\n",
    "        target_names += ['none']\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "\n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true_mapped, y_pred=y_pred_mapped,\n",
    "                                         target_names=target_names,\n",
    "                                         labels=labels, zero_division=0.0)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true_mapped, y_pred=y_pred_mapped,\n",
    "                                   labels=labels)\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9d379",
   "metadata": {},
   "source": [
    "Evaluate accuracy before fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b23bec7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 695/695 [05:45<00:00,  2.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Manuf    217\n",
       "NoDur    184\n",
       "HiTec    109\n",
       "Other     65\n",
       "none      54\n",
       "Hlth      24\n",
       "Utils     15\n",
       "Telcm     14\n",
       "Shops      8\n",
       "Enrgy      4\n",
       "Durbl      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict(X_test, model, tokenizer)\n",
    "Series(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "344078f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.203\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Durbl       0.00      0.00      0.00        33\n",
      "       Enrgy       0.50      0.10      0.17        20\n",
      "       HiTec       0.25      0.19      0.22       139\n",
      "        Hlth       0.88      0.13      0.22       164\n",
      "       Manuf       0.22      0.70      0.34        69\n",
      "       NoDur       0.03      0.21      0.06        28\n",
      "       Other       0.25      0.10      0.15       153\n",
      "       Shops       0.75      0.10      0.17        62\n",
      "       Telcm       0.36      0.56      0.43         9\n",
      "       Utils       0.67      0.56      0.61        18\n",
      "        none       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.20       695\n",
      "   macro avg       0.35      0.24      0.21       695\n",
      "weighted avg       0.44      0.20      0.21       695\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0  0  2  0 18 10  2  1  0  0  0]\n",
      " [ 0  2  0  0  8  8  2  0  0  0  0]\n",
      " [ 0  0 27  0 43 38 18  0  8  4  1]\n",
      " [ 0  0 73 21 15 22 11  0  1  1 20]\n",
      " [ 0  0  3  0 48 12  5  1  0  0  0]\n",
      " [ 1  0  0  0 17  6  4  0  0  0  0]\n",
      " [ 0  0  4  1 41 64 16  0  0  0 27]\n",
      " [ 0  0  0  1 26 16  7  6  0  0  6]\n",
      " [ 0  0  0  0  0  4  0  0  5  0  0]\n",
      " [ 0  2  0  1  1  4  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d4db01",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463a3361",
   "metadata": {},
   "source": [
    "Create the model trainer using training arguments, a LoRA configuration, and a dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d6c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_data,\n",
    "    peft_config=peft_config,\n",
    "#    dataset_text_field=\"text\",\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49fcfff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ BEFORE TRAINING ------\n",
      "Total memory: 15.74 GB\n",
      "Reserved memory: 11.04 GB\n",
      "Allocated memory: 8.22 GB\n",
      "Free memory: 4.70 GB\n",
      "{'loss': 1.1984, 'grad_norm': 0.1371612697839737, 'learning_rate': 0.0001670747898848231, 'num_tokens': 1091299.0, 'mean_token_accuracy': 0.7146163220703602, 'epoch': 0.5755395683453237}\n",
      "{'loss': 1.1205, 'grad_norm': 0.16719305515289307, 'learning_rate': 8.029070592154895e-05, 'num_tokens': 2179799.0, 'mean_token_accuracy': 0.7273549642927366, 'epoch': 1.1496402877697842}\n",
      "{'loss': 1.034, 'grad_norm': 0.19266854226589203, 'learning_rate': 9.47361624665869e-06, 'num_tokens': 3270551.0, 'mean_token_accuracy': 0.7437317748367787, 'epoch': 1.725179856115108}\n",
      "{'train_runtime': 9602.662, 'train_samples_per_second': 0.579, 'train_steps_per_second': 0.072, 'train_loss': 1.103452600044888, 'num_tokens': 3784895.0, 'mean_token_accuracy': 0.7479746815689067, 'epoch': 1.99568345323741}\n"
     ]
    }
   ],
   "source": [
    "# Initiate model training\n",
    "cuda_memory('before training')\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=RESUME_FROM_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0930860e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ AFTER TRAINING ------\n",
      "9602.662 seconds used for training.\n",
      "Total memory: 15.74 GB\n",
      "Reserved memory: 14.43 GB\n",
      "Allocated memory: 8.26 GB\n",
      "Free memory: 1.31 GB\n"
     ]
    }
   ],
   "source": [
    "# Save trained model and tokenizer\n",
    "model.config.use_cache = True\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "cuda_memory('after training', trainer_stats=trainer_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee7b11",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "748c84f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/695 [00:00<?, ?it/s]/home/terence/env3.11/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 695/695 [08:21<00:00,  1.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Hlth     168\n",
       "Other    156\n",
       "HiTec    140\n",
       "Manuf     59\n",
       "Shops     59\n",
       "NoDur     34\n",
       "Durbl     29\n",
       "Enrgy     21\n",
       "Utils     19\n",
       "Telcm     10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict(X_test, model, tokenizer, verbose=False)\n",
    "Series(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc0725f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.829\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Durbl       0.83      0.73      0.77        33\n",
      "       Enrgy       0.90      0.95      0.93        20\n",
      "       HiTec       0.79      0.80      0.80       139\n",
      "        Hlth       0.89      0.91      0.90       164\n",
      "       Manuf       0.80      0.68      0.73        69\n",
      "       NoDur       0.59      0.71      0.65        28\n",
      "       Other       0.85      0.86      0.85       153\n",
      "       Shops       0.83      0.79      0.81        62\n",
      "       Telcm       0.90      1.00      0.95         9\n",
      "       Utils       0.84      0.89      0.86        18\n",
      "\n",
      "    accuracy                           0.83       695\n",
      "   macro avg       0.82      0.83      0.83       695\n",
      "weighted avg       0.83      0.83      0.83       695\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 24   0   6   0   1   1   0   1   0   0]\n",
      " [  0  19   0   0   1   0   0   0   0   0]\n",
      " [  0   2 111   7   2   2  12   2   1   0]\n",
      " [  0   0   9 149   1   1   3   1   0   0]\n",
      " [  5   0   4   2  47   6   2   2   0   1]\n",
      " [  0   0   0   1   2  20   2   3   0   0]\n",
      " [  0   0  10   4   5   1 132   1   0   0]\n",
      " [  0   0   0   4   0   3   4  49   0   2]\n",
      " [  0   0   0   0   0   0   0   0   9   0]\n",
      " [  0   0   0   1   0   0   1   0   0  16]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830df261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge and save model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "cuda_memory('after empty')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2634dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload base model and tokenizer to cpu\n",
    "device_map = \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        return_dict=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map, # \"cpu\",   # \"auto\",\n",
    "        trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb84429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge adapter with base model\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(base_model_reload, output_dir, device_map=device_map)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba062556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged model\n",
    "model.save_pretrained(model_dir)\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bbffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload nerged model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        return_dict=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",   # 'cpu',\n",
    "        trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac950c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check it is working\n",
    "y_pred = predict(X_test, model, tokenizer)\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9f9e6",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "Philipp Krähenbühl, 2025, \"AI395T Advances in Deep Learning course materials\", retrieved from https://ut.philkr.net/advances_in_deeplearning/\n",
    "\n",
    "Tim Dettmers, \"Bitsandbytes: 8-bit Optimizers and Quantization for PyTorch\", 2022.\n",
    "GitHub repository: https://github.com/TimDettmers/bitsandbytes\n",
    "\n",
    "https://www.datacamp.com/tutorial/fine-tuning-llama-3-1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "env3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
