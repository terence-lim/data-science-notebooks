{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26f070a0",
   "metadata": {},
   "source": [
    "# 10-K Business Description\n",
    "\n",
    "_UNDER CONSTRUCTION_\n",
    "\n",
    "- Spacy\n",
    "- Syntactic analysis, POS tags, named entity recognition\n",
    "- Logistic regression, Perceptron, stochastic gradient descent\n",
    "- Growth and Value stocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a757bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import gzip\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from wordcloud import WordCloud\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from finds.database.sql import SQL\n",
    "from finds.database.redisdb import Redis\n",
    "from finds.database.mongodb import MongoDB\n",
    "from finds.structured.crsp import CRSP\n",
    "from finds.structured.pstat import PSTAT\n",
    "from finds.structured.benchmarks import Benchmarks\n",
    "from finds.structured.signals import Signals\n",
    "from finds.backtesting.backtest import BackTest\n",
    "from finds.busday import BusDay\n",
    "from finds.unstructured import Unstructured\n",
    "from finds.unstructured.store import Store\n",
    "from finds.readers.sectoring import Sectoring\n",
    "from finds.readers.edgar import Edgar\n",
    "from finds.misc.show import Show\n",
    "from finds.plots import plot_date\n",
    "from secret import credentials, paths\n",
    "# %matplotlib qt\n",
    "VERBOSE = 0\n",
    "show = Show(ndigits=4, latex=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf1d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = SQL(**credentials['sql'], verbose=VERBOSE)\n",
    "user = SQL(**credentials['user'], verbose=VERBOSE)\n",
    "bd = BusDay(sql)\n",
    "rdb = Redis(**credentials['redis'])\n",
    "crsp = CRSP(sql, bd, rdb, verbose=VERBOSE)\n",
    "pstat = PSTAT(sql, bd, verbose=VERBOSE)\n",
    "bench = Benchmarks(sql, bd, verbose=VERBOSE)\n",
    "ed = Edgar(paths['10X'], zipped=True, verbose=VERBOSE)\n",
    "imgdir = paths['images'] / 'edgar'\n",
    "store = Store(paths['scratch'], ext='pkl')\n",
    "item, form = 'bus10K', '10-K'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fcc81a",
   "metadata": {},
   "source": [
    "# 10-K Business Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677f704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve universe of stocks\n",
    "# 5-year growth and book-to-price, by 2022, 1997, 1972\n",
    "# NYSE top-half market cap\n",
    "univ = crsp.get_universe(20181231)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c368917",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = crsp.build_lookup('permno', 'comnam', fillna=\"\")  # company name\n",
    "comnam = lookup(univ.index)\n",
    "univ['comnam'] = comnam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb764055",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_sic = pstat.build_lookup('lpermno', 'sic', fillna=0)     # sic from PSTAT\n",
    "sic_ = Series(lookup_sic(univ.index, date=20181231), univ.index)\n",
    "univ['siccd'] = univ['siccd'].where(sic_.isin([0, 9999]), sic_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c30462",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_naics = pstat.build_lookup('lpermno', 'naics', fillna=0) # naics from PSTAT\n",
    "naics_ = Series(lookup_naics(univ.index, date=20181231), univ.index)\n",
    "univ['naics'] = univ['naics'].where(sic_.isin([0, 9999]), naics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee9ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve business descriptions text; extract nouns from POS tags\n",
    "nlp = spacy.load(\"en_core_web_lg\")   # Load a spaCy language pipeline\n",
    "if 'bus' not in store:   # store processed text if necessary\n",
    "    rows = DataFrame(ed.open(form=form, item=item))  # open bus10K archive\n",
    "    bus = {}\n",
    "    restart = 0\n",
    "    for i, permno in tqdm(enumerate(univ.index)):\n",
    "        found = rows[rows['permno'].eq(permno) &\n",
    "                     rows['date'].between(20190101, 20190331)]\n",
    "        if len(found) and i >= restart:\n",
    "            doc = nlp(ed[found.iloc[0]['pathname']][:nlp.max_length].lower())\n",
    "            bus[permno] = \" \".join([re.sub(\"[^a-zA-Z]+\", \"\", token.lemma_)\n",
    "                                    for token in doc if token.pos_ in ['NOUN']\n",
    "                                    and len(token.lemma_) > 2])\n",
    "    store.dump(bus, 'bus')   # serialize\n",
    "bus = store.load('bus')\n",
    "keys = list(bus.keys())\n",
    "corpus = list(bus.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccc799c",
   "metadata": {},
   "source": [
    "# Bag-of-words Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7905cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=10)\n",
    "tfidf = vectorizer.fit_transform(corpus)\n",
    "X = tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b22bc5c",
   "metadata": {},
   "source": [
    "__Retrieve Fama-French sector scheme__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4571c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate codes49 industry, company name, and legacy sector\n",
    "codes = Sectoring(sql, scheme='codes49', fillna=\"\")     # codes49 industry\n",
    "sic = Sectoring(sql, scheme='sic', fillna=0)  \n",
    "codes49 = Series(codes[univ['siccd']])\n",
    "replace = univ['siccd'].isin([0, 9999]).values\n",
    "codes49[replace] = codes[sic[univ.loc[replace, 'naics']]]\n",
    "univ['industry'] = codes49.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d26b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes12 = Sectoring(sql, scheme='codes12', fillna=\"\")  # [5,10,12,17,30,38,48,49]\n",
    "sic = Sectoring(sql, scheme='sic', fillna=0)    # cross-walk naics to sic\n",
    "legacy = Series(codes12[univ['siccd']])         # convert sic to legacy sector\n",
    "replace = (legacy.eq(\"\").values | univ['siccd'].isin([0, 9999]).values)\n",
    "legacy[replace] = codes12[sic[univ.loc[replace, 'naics']]] # convert naics\n",
    "univ['legacy'] = legacy.tolist()\n",
    "y = univ['legacy'].reindex(keys)\n",
    "print(y.groupby(y).count().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049997f",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "The logistic regression update is:\n",
    "- $P(y=1 | x) \\leftarrow 1/(1 + e^{-w x})$\n",
    "\n",
    "- $w \\leftarrow w + \\alpha ~ x ~(1 - P(y=1|x))$ if $y = 1$\n",
    "\n",
    "- $w \\leftarrow w - \\alpha ~ x ~(1 - P(y=0|x))$ if $y = 0$\n",
    "\n",
    "$\\Rightarrow w \\leftarrow w + \\alpha ~ x ~(y - P(y=1|x))$ where $y \\in \\{0,~1\\}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ae1f9",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "The perceptron update is:\n",
    "\n",
    "- $\\hat{y} \\leftarrow \\mathrm{sign}(w x)$\n",
    "\n",
    "- $w \\leftarrow w + \\alpha x$ if $y = +1$ and $y \\ne \\hat{y}$\n",
    "- $w \\leftarrow w - \\alpha x$ if $y = -1$  and $y \\ne \\hat{y}$\n",
    "\n",
    "$\\Rightarrow w \\leftarrow w + \\alpha~x~(y - \\hat{y})/2$  where $y \\in \\{-1,~+1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df2d30",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "- confusion matrix, precision, recall\n",
    "- auc, roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da220743",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion Matrix\n",
    "print(confusion_matrix(y, res.clf.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a286a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ConfusionMatrixDisplay.from_predictions(y, res.clf.predict(X), ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.savefig(imgdir / 'logistic_cf.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f1ed35",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))  #  accuracy vs alpha\n",
    "ax.semilogx(res.Cs, res.valid_accuracy) #, drawstyle=\"steps-post\")\n",
    "ax.semilogx(res.Cs, res.train_accuracy) #, drawstyle=\"steps-post\")\n",
    "argmax = np.argmax(res.valid_accuracy)\n",
    "ax.annotate(f\"{res.valid_accuracy[argmax]:.4f}\",\n",
    "            xy=(res.Cs[argmax], res.valid_accuracy[argmax]))\n",
    "ax.plot(res.Cs[argmax], res.valid_accuracy[argmax], \"o\")\n",
    "ax.set_xlabel(\"Regularization parameter (C)\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(f\"Softmax Regression: Accuracy vs Complexity\")\n",
    "ax.legend(['Cross-Validation Accuracy', 'Training Accuracy'])\n",
    "plt.tight_layout()\n",
    "plt.savefig(imgdir / 'logistic.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29892e54",
   "metadata": {},
   "source": [
    "## Feature importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 20 \n",
    "words = {}\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic, lab in enumerate(res.clf.classes_):\n",
    "    importance = res.clf.coef_[topic, :]\n",
    "    words[lab] = [feature_names[i]\n",
    "                  for i in importance.argsort()[:-top_n-1:-1]]\n",
    "    freqs = {feature_names[i]: importance[i]\n",
    "             for i in importance.argsort()[:-top_n-1:-1]}\n",
    "    fig, ax = plt.subplots(figsize=(3.5, 3), clear=True)\n",
    "    wc = WordCloud(height=500, width=500, colormap='cool')\n",
    "    ax.imshow(wc.generate_from_frequencies(freqs))\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(lab)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(imgdir / f\"logistic_wc{topic}.jpg\")\n",
    "out = DataFrame.from_dict(words, orient='columns')\n",
    "show(out, index=False, **SHOW)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
