{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b50d42bd",
   "metadata": {},
   "source": [
    "# LLM Finetuning\n",
    "\n",
    "Concepts:\n",
    "- Industry Classification\n",
    "- Unsloth APIs\n",
    "- Performance Efficient Fine-tuning\n",
    "- Supervised Fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7c2d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/terence/env3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from tqdm import tqdm\n",
    "from finds.database import SQL, RedisDB\n",
    "from finds.unstructured import Edgar\n",
    "from finds.structured import BusDay, CRSP, PSTAT\n",
    "from finds.readers import Sectoring\n",
    "from finds.utils import Store\n",
    "from secret import credentials, paths\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46797e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last FamaFrench Date 2024-04-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "VERBOSE = 0\n",
    "sql = SQL(**credentials['sql'], verbose=VERBOSE)\n",
    "bd = BusDay(sql)\n",
    "rdb = RedisDB(**credentials['redis'])\n",
    "crsp = CRSP(sql, bd, rdb, verbose=VERBOSE)\n",
    "pstat = PSTAT(sql, bd, verbose=VERBOSE)\n",
    "ed = Edgar(paths['10X'], zipped=True, verbose=0)\n",
    "store = Store(paths['scratch'], ext='pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e3f41d",
   "metadata": {},
   "source": [
    "Load 10-K business description text for industry classification task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db5546d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve universe of stocks\n",
    "univ = crsp.get_universe(bd.endmo(20221231))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "903bff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup company names\n",
    "comnam = crsp.build_lookup(source='permno', target='comnam', fillna=\"\")\n",
    "univ['comnam'] = comnam(univ.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b614de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup ticker symbols\n",
    "ticker = crsp.build_lookup(source='permno', target='ticker', fillna=\"\")\n",
    "univ['ticker'] = ticker(univ.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4039abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup sic codes from Compustat, and map to FF 10-sector code\n",
    "sic = pstat.build_lookup(source='lpermno', target='sic', fillna=0)\n",
    "industry = Series(sic[univ.index], index=univ.index)\n",
    "industry = industry.where(industry > 0, univ['siccd'])\n",
    "sectors = Sectoring(sql, scheme='codes10', fillna='')   # supplement from crosswalk\n",
    "univ['sector'] = sectors[industry]\n",
    "#fullnames = Series({'Durbl': \"Durable\", 'Enrgy': \"Energy\",\n",
    "#                    'HiTec': \"Technology\", 'Hlth': \"Healthcare\",\n",
    "#                    'Manuf': \"Manufacturing\", 'NoDur': \"Nondurable\",\n",
    "#                    'Other': \"Other\", 'Shops': \"Retail\",\n",
    "#                    'Telcm': \"Telecommunications\", 'Utils': \"Utilities\"})\n",
    "#univ['sector'] = fullnames[sectors[industry]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4daa7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same permnos from other earlier experiments\n",
    "permnos = list(store.load('nouns').keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62e1857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve 2023 bus10K's\n",
    "item, form = 'bus10K', '10-K'\n",
    "rows = DataFrame(ed.open(form=form, item=item))\n",
    "found = rows[rows['date'].between(20230101, 20231231)]\\\n",
    "             .drop_duplicates(subset=['permno'], keep='last')\\\n",
    "             .set_index('permno')\\\n",
    "             .reindex(permnos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e33a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sector</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hlth</th>\n",
       "      <td>881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HiTec</th>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manuf</th>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shops</th>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Durbl</th>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NoDur</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Enrgy</th>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Utils</th>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Telcm</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count\n",
       "sector       \n",
       "Hlth      881\n",
       "Other     762\n",
       "HiTec     706\n",
       "Manuf     344\n",
       "Shops     321\n",
       "Durbl     164\n",
       "NoDur     145\n",
       "Enrgy      94\n",
       "Utils      92\n",
       "Telcm      50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split documents into train/test sets\n",
    "labels = univ.loc[permnos, 'sector']\n",
    "train_index, test_index = train_test_split(permnos,\n",
    "                                           stratify=labels,\n",
    "                                           random_state=42,\n",
    "                                           test_size=0.2)\n",
    "Series(labels).value_counts().rename('count').to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "828d1cd8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3559 2847 712 ['Durbl' 'Enrgy' 'HiTec' 'Hlth' 'Manuf' 'NoDur' 'Other' 'Shops' 'Telcm'\n",
      " 'Utils']\n"
     ]
    }
   ],
   "source": [
    "# helper to decode class label in text\n",
    "class_labels = np.unique(labels)\n",
    "def decode_label(text):\n",
    "    \"\"\"Extract label from output string text\"\"\"\n",
    "    for lab in sorted(class_labels, key=lambda x: -len(x)): # longest strings first  \n",
    "        if lab.lower() in text.lower():\n",
    "            return lab\n",
    "    return \"\"\n",
    "print(len(labels), len(train_index), len(test_index), class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd2c897",
   "metadata": {},
   "source": [
    "## Unsloth APIs\n",
    "\n",
    "Unslothâ€™s library uses several advanced techniques to make training large language models (LLMs) much faster and more efficient, for example, by optimizing matrix multiplications, which are a key part of LLM training, by chaining them together efficiently.\n",
    "\n",
    "Installing unsloth:\n",
    "\n",
    "`!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"`\n",
    "\n",
    "`major_version, minor_version = torch.cuda.get_device_capability()` - must install separately since torch 2.2.1 breaks packages \n",
    "\n",
    "- if major_version >= 8 (new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40))\n",
    "\n",
    "  `!pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes`\n",
    "\n",
    "- else: older GPUs (V100, Tesla T4, RTX 20xx)\n",
    "\n",
    "  `!pip install --no-deps xformers trl peft accelerate bitsandbytes`\n",
    "\n",
    "```\n",
    "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
    "PyTorch 2.3.0+cu121 with CUDA 1201 (you have 2.2.1+cu121)\n",
    "Python  3.11.9 (you have 3.11.9)\n",
    "Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
    "```\n",
    "\n",
    "- install the corresponding xformers (https://anaconda.org/xformers/xformers/files)\n",
    "\n",
    "```!pip3 install torch==2.2.1+cu121 --index-url https://download.pytorch.org/whl/cu121 --force-reinstall\n",
    "   !pip install xformers==0.0.25  # for torch 2.2.1+cu121\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f5868f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_MODEL = True\n",
    "model_id =\"unsloth/llama-3-8b-bnb-4bit\"\n",
    "savedir = str(paths['scratch'] / 'finetuned_model')\n",
    "output_dir = str(paths['scratch'] / \"outputs\")\n",
    "max_seq_length = 8192\n",
    "dtype = None # None for auto. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c23a29e2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3080 Laptop GPU. Max memory: 15.739 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# 4bit pre quantized models from unsloth for 4x faster downloading + no OOMs\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id if NEW_MODEL else savedir,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # if using gated models like meta-llama/Meta-Llama-3-8B\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8620f5",
   "metadata": {},
   "source": [
    "## Low-rank adaptation\n",
    "\n",
    "Parameter-efficient fine-tuning (PEFT) methods significantly decrease the computational and storage costs for large language models by only fine-tuning a small number of (extra) model parameters instead of all the model's parameters. Low-rank adaptation (LoRA) reduces the number of trainable parameters by learning pairs of rank-decompostion matrices while freezing the original weights.\n",
    "\n",
    "The `unsloth` library replaces HuggingFace's `peft`:\n",
    "\n",
    "```\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    savedir, # YOUR MODEL YOU USED FOR TRAINING\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"classifier\"],\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edd7ad3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Add LoRA adapters so we only need to update less than 10% of all parameters\n",
    "if NEW_MODEL:\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                          \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "        lora_alpha = 16,\n",
    "        lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "        bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "        use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    "        random_state = 3407,\n",
    "        use_rslora = False,  # supports rank stabilized LoRA\n",
    "        loftq_config = None, # supports LoftQ\n",
    "    )\n",
    "\n",
    "# [NOTE] To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
    "# [NOTE] Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772e0903",
   "metadata": {},
   "source": [
    "## HuggingFace datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abf816de",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token   # Must add EOS_TOKEN\n",
    "def generate_prompt(text, label=''):\n",
    "    return f\"\"\"\n",
    "Below is an instruction that describes a task. \n",
    "Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "The text of a business description can be classified in industry categories like below.\n",
    "\n",
    "\"{'\" or \"'.join(class_labels)}\".\n",
    "\n",
    "In one word, please respond with the industry classification category \n",
    "of the following text delimited in triple quotes.\n",
    "\n",
    "'''{text}'''\n",
    "\n",
    "### Response: \n",
    "{label} \"\"\" + EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4af90569",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "MAX_CHARS = 4096\n",
    "def data_generator():\n",
    "    for permno in train_index:\n",
    "        text = ed[found.loc[permno, 'pathname']].replace('\\n','').lower()[:MAX_CHARS]\n",
    "        yield {\"text\": generate_prompt(text, label=univ.loc[permno, 'sector'])}\n",
    "dataset = Dataset.from_generator(data_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9001cd72",
   "metadata": {},
   "source": [
    "## Supervised fine-tuning\n",
    "\n",
    "Supervised Fine-Tuning adapts a pre-trained model to a specific task, by training the model on a new labeled dataset to predict the correct label for each input.\n",
    "\n",
    "The TRL library provides the SFTTrainer class, which is designed to facilitate the SFT process. This class accepts a column in your training dataset that contains the prompt constructed from system instructions, questions, and answers.\n",
    "\n",
    "https://huggingface.co/docs/trl/sft_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e4fe0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2847/2847 [00:02<00:00, 1058.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS = 16\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,  # 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = -1, # 60, -1\n",
    "        num_train_epochs = MAX_EPOCHS,  # default 3\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_strategy = \"steps\" if VERBOSE else \"no\",  # \"epoch\", \"no\", \"steps\"\n",
    "        logging_steps = 1,  # defaults to 500 if logging_strategy=\"steps\"\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        save_strategy=\"epoch\",      # Save the model checkpoint every epoch\n",
    "        output_dir = output_dir,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccb4b904",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3080 Laptop GPU. Max memory = 15.739 GB.\n",
      "5.594 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4e01b51",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Helpers to generate input tokens and response\n",
    "def generate_inputs(permno):\n",
    "    \"\"\"Transform text corresponding to permno into tokenized input\"\"\"\n",
    "    text = ed[found.loc[permno, 'pathname']].replace('\\n','').lower()[:MAX_CHARS]\n",
    "    inputs = tokenizer([generate_prompt(text)], return_tensors=\"pt\").to(\"cuda\")\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc2bb70c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_output(inputs):\n",
    "    \"\"\"Generate response given tokenized input, and return decoded output text\"\"\"\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=16,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        #do_sample=True,\n",
    "        #temperature=0.01, #0.6,\n",
    "        #top_p=0.9,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    response = outputs[0][len(inputs[0]):]\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b3e3965",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_output(permnos, verbose=VERBOSE):\n",
    "    \"\"\"Return predicted and true labels\"\"\"\n",
    "    y_pred, y_true = [], []\n",
    "    for permno in permnos:\n",
    "        inputs = generate_inputs(permno)\n",
    "        output = generate_output(inputs).replace('\\n', ' ')\n",
    "        label = decode_label(output)\n",
    "        gold = univ.loc[permno, 'sector']\n",
    "        y_pred.append(label)\n",
    "        y_true.append(gold)\n",
    "        if verbose:\n",
    "            print(permno, gold, '  [', label, ']   ', output)\n",
    "    return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58844e3c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Test Set) before fine-tuning:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   0.00      0.00      0.00         0\n",
      "       Durbl       0.00      0.00      0.00        33\n",
      "       Enrgy       0.00      0.00      0.00        19\n",
      "       HiTec       0.00      0.00      0.00       141\n",
      "        Hlth       0.50      0.01      0.01       176\n",
      "       Manuf       0.12      0.06      0.08        69\n",
      "       NoDur       0.01      0.03      0.02        29\n",
      "       Other       0.00      0.00      0.00       153\n",
      "       Shops       0.00      0.00      0.00        64\n",
      "       Telcm       0.00      0.00      0.00        10\n",
      "       Utils       0.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.01       712\n",
      "   macro avg       0.06      0.01      0.01       712\n",
      "weighted avg       0.14      0.01      0.01       712\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/terence/env3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/terence/env3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/terence/env3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/terence/env3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/terence/env3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/terence/env3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate before fine-tuning\n",
    "if True:\n",
    "    FastLanguageModel.for_inference(model)   # Enable native 2x faster inference\n",
    "    y_pred, y_true = evaluate_output(test_index)\n",
    "\n",
    "    # generate classification report\n",
    "    report = metrics.classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    print(f\"Classification Report (Test Set) before fine-tuning:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78abcc1a",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7725b81",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,847 | Num Epochs = 16\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 5,696\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5696/5696 [20:39:52<00:00, 13.06s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 74392.7458, 'train_samples_per_second': 0.612, 'train_steps_per_second': 0.077, 'train_loss': 0.494993617025654, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5783a678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74392.7458 seconds used for training.\n",
      "Peak reserved memory = 9.1 GB.\n",
      "Peak reserved memory for training = 3.35 GB.\n",
      "Peak reserved memory % of max memory = 57.818 %.\n",
      "Peak reserved memory for training % of max memory = 21.285 %.\n"
     ]
    }
   ],
   "source": [
    "# Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56e208fb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Save fine-tuned model\n",
    "# **[NOTE]** This ONLY saves the LoRA adapters, and not the full model.\n",
    "if True:\n",
    "    model.save_pretrained(savedir) # Local saving\n",
    "    tokenizer.save_pretrained(savedir)\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef341c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on train split\n",
    "if False:\n",
    "    FastLanguageModel.for_inference(model)   # Enable native 2x faster inference\n",
    "    y_pred, y_true = evaluate_output(train_index)\n",
    "\n",
    "    # generate classification report\n",
    "    report = metrics.classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    print(f\"Classification Report (Training set):\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25bd9d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 712/712 [09:10<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Test Set) after fine-tuning:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   0.00      0.00      0.00         0\n",
      "       Durbl       0.33      0.61      0.43        33\n",
      "       Enrgy       1.00      0.47      0.64        19\n",
      "       HiTec       0.74      0.59      0.66       141\n",
      "        Hlth       0.96      0.89      0.92       176\n",
      "       Manuf       0.70      0.71      0.71        69\n",
      "       NoDur       0.70      0.55      0.62        29\n",
      "       Other       0.80      0.70      0.75       153\n",
      "       Shops       0.77      0.78      0.78        64\n",
      "       Telcm       1.00      0.90      0.95        10\n",
      "       Utils       0.65      0.94      0.77        18\n",
      "\n",
      "    accuracy                           0.72       712\n",
      "   macro avg       0.70      0.65      0.66       712\n",
      "weighted avg       0.79      0.72      0.75       712\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/terence/env3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/terence/env3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/terence/env3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Run inference on test split\n",
    "if True:\n",
    "    FastLanguageModel.for_inference(model)   # Enable native 2x faster inference\n",
    "    y_pred, y_true = evaluate_output(test_index)\n",
    "\n",
    "    # generate classification report\n",
    "    report = metrics.classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    print(f\"Classification Report (Test Set) after fine-tuning:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d8f910",
   "metadata": {},
   "source": [
    "Display a few responses from fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec59d3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17147 Hlth   [ Hlth ]       Hlth \n",
      "22952 Other   [ Other ]     Other \n",
      "14547 HiTec   [ HiTec ]     HiTec \n",
      "88284 Hlth   [ Hlth ]       Hlth \n",
      "88873 HiTec   [ Other ]     Hi there,   Other has the highest weight so that's the one\n",
      "13777 HiTec   [ Other ]     Hi there,   Other \n",
      "89968 Other   [ Other ]     Other \n",
      "16471 Other   [ Other ]     Other \n",
      "21933 Hlth   [ Hlth ]       Hlth \n",
      "90542 Hlth   [ Hlth ]       Hlth \n",
      "22260 Manuf   [ NoDur ]     NoDur \n",
      "16401 Enrgy   [ Enrgy ]     Enrgy \n",
      "14329 Shops   [ Shops ]     Shops \n",
      "79698 Hlth   [  ]     Hi there,   I'm a business description writer, and based on\n",
      "92443 Other   [ Other ]     Other \n",
      "19085 Other   [  ]     Bank \n",
      "86810 Other   [ NoDur ]     NoDur \n",
      "18960 Hlth   [ Hlth ]       Hlth \n",
      "15943 HiTec   [  ]     Hi there,   The industry classification category of our business description can be\n",
      "17106 HiTec   [ HiTec ]     HiTec \n",
      "76076 HiTec   [ HiTec ]     HiTec \n",
      "20412 Manuf   [ Manuf ]     Manuf \n",
      "13812 HiTec   [ HiTec ]     HiTec \n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_output(test_index[::32], verbose=1)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "env3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
